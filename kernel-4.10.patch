--- nvidia-kmod-378.13-x86_64/kernel/common/inc/nv-linux.h.orig	2017-02-20 21:51:57.092068826 +0000
+++ nvidia-kmod-378.13-x86_64/kernel/common/inc/nv-linux.h	2017-02-20 22:20:25.828495923 +0000
@@ -294,11 +294,13 @@
 
 extern int nv_pat_mode;
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
 #if defined(CONFIG_HOTPLUG_CPU)
 #define NV_ENABLE_HOTPLUG_CPU
 #include <linux/cpu.h>              /* CPU hotplug support              */
 #include <linux/notifier.h>         /* struct notifier_block, etc       */
 #endif
+#endif
 
 #if (defined(CONFIG_I2C) || defined(CONFIG_I2C_MODULE))
 #include <linux/i2c.h>
--- nvidia-kmod-378.13-x86_64/kernel/nvidia/nv-p2p.c.orig	2017-02-20 21:52:16.479919933 +0000
+++ nvidia-kmod-378.13-x86_64/kernel/nvidia/nv-p2p.c	2017-02-20 22:28:13.115724332 +0000
@@ -146,7 +146,11 @@
 int nvidia_p2p_get_pages(
     uint64_t p2p_token,
     uint32_t va_space,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     uint64_t virtual_address,
+#else
+    uint64_t address,
+#endif
     uint64_t length,
     struct nvidia_p2p_page_table **page_table,
     void (*free_callback)(void * data),
@@ -211,7 +215,11 @@
     }
 
     status = rm_p2p_get_pages(sp, p2p_token, va_space,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
             virtual_address, length, physical_addresses, wreqmb_h,
+#else
+            address, length, physical_addresses, wreqmb_h,
+#endif
             rreqmb_h, &entries, &gpu_uuid, *page_table,
             free_callback, data);
     if (status != NV_OK)
@@ -286,7 +294,11 @@
 
     if (bGetPages)
     {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
         rm_p2p_put_pages(sp, p2p_token, va_space, virtual_address,
+#else
+        rm_p2p_put_pages(sp, p2p_token, va_space, address,
+#endif
                 gpu_uuid, *page_table);
     }
 
@@ -329,7 +341,11 @@
 int nvidia_p2p_put_pages(
     uint64_t p2p_token,
     uint32_t va_space,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     uint64_t virtual_address,
+#else
+    uint64_t address,
+#endif
     struct nvidia_p2p_page_table *page_table
 )
 {
@@ -343,7 +359,11 @@
         return rc;
     }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     status = rm_p2p_put_pages(sp, p2p_token, va_space, virtual_address,
+#else
+    status = rm_p2p_put_pages(sp, p2p_token, va_space, address,
+#endif
             page_table->gpu_uuid, page_table);
     if (status == NV_OK)
         nvidia_p2p_free_page_table(page_table);
--- nvidia-kmod-378.13-x86_64/kernel/nvidia-drm/nvidia-drm-fence.c.orig	2017-02-21 22:10:35.232680514 +0000
+++ nvidia-kmod-378.13-x86_64/kernel/nvidia-drm/nvidia-drm-fence.c	2017-02-21 22:11:12.442710689 +0000
@@ -31,7 +31,11 @@
 
 #if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
 struct nv_fence {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     struct fence base;
+#else
+    struct dma_fence base;
+#endif
     spinlock_t lock;
 
     struct nvidia_drm_device *nv_dev;
@@ -51,7 +55,11 @@
 
 static const char *nvidia_drm_gem_prime_fence_op_get_driver_name
 (
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     struct fence *fence
+#else
+    struct dma_fence *fence
+#endif
 )
 {
     return "NVIDIA";
@@ -59,7 +67,11 @@
 
 static const char *nvidia_drm_gem_prime_fence_op_get_timeline_name
 (
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     struct fence *fence
+#else
+    struct dma_fence *fence
+#endif
 )
 {
     return "nvidia.prime";
@@ -67,7 +79,11 @@
 
 static bool nvidia_drm_gem_prime_fence_op_signaled
 (
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     struct fence *fence
+#else
+    struct dma_fence *fence
+#endif
 )
 {
     struct nv_fence *nv_fence = container_of(fence, struct nv_fence, base);
@@ -99,7 +115,11 @@
 
 static bool nvidia_drm_gem_prime_fence_op_enable_signaling
 (
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     struct fence *fence
+#else
+    struct dma_fence *fence
+#endif
 )
 {
     bool ret = true;
@@ -107,7 +127,11 @@
     struct nvidia_drm_gem_object *nv_gem = nv_fence->nv_gem;
     struct nvidia_drm_device *nv_dev = nv_fence->nv_dev;
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     if (fence_is_signaled(fence))
+#else
+    if (dma_fence_is_signaled(fence))
+#endif
     {
         return false;
     }
@@ -136,7 +160,11 @@
     }
 
     nv_gem->fenceContext.softFence = fence;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     fence_get(fence);
+#else
+    dma_fence_get(fence);
+#endif
 
 unlock_struct_mutex:
     mutex_unlock(&nv_dev->dev->struct_mutex);
@@ -146,7 +174,11 @@
 
 static void nvidia_drm_gem_prime_fence_op_release
 (
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     struct fence *fence
+#else
+    struct dma_fence *fence
+#endif
 )
 {
     struct nv_fence *nv_fence = container_of(fence, struct nv_fence, base);
@@ -155,7 +187,11 @@
 
 static signed long nvidia_drm_gem_prime_fence_op_wait
 (
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     struct fence *fence,
+#else
+    struct dma_fence *fence,
+#endif
     bool intr,
     signed long timeout
 )
@@ -170,12 +206,20 @@
      * that it should never get hit during normal operation, but not so long
      * that the system becomes unresponsive.
      */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     return fence_default_wait(fence, intr,
+#else
+    return dma_fence_default_wait(fence, intr,
+#endif
                               (timeout == MAX_SCHEDULE_TIMEOUT) ?
                                   msecs_to_jiffies(96) : timeout);
 }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
 static const struct fence_ops nvidia_drm_gem_prime_fence_ops = {
+#else
+static const struct dma_fence_ops nvidia_drm_gem_prime_fence_ops = {
+#endif
     .get_driver_name = nvidia_drm_gem_prime_fence_op_get_driver_name,
     .get_timeline_name = nvidia_drm_gem_prime_fence_op_get_timeline_name,
     .signaled = nvidia_drm_gem_prime_fence_op_signaled,
@@ -285,7 +329,11 @@
     bool force
 )
 {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     struct fence *fence = nv_gem->fenceContext.softFence;
+#else
+    struct dma_fence *fence = nv_gem->fenceContext.softFence;
+#endif
 
     WARN_ON(!mutex_is_locked(&nv_dev->dev->struct_mutex));
 
@@ -301,10 +349,18 @@
 
         if (force || nv_fence_ready_to_signal(nv_fence))
         {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
             fence_signal(&nv_fence->base);
+#else
+            dma_fence_signal(&nv_fence->base);
+#endif
 
             nv_gem->fenceContext.softFence = NULL;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
             fence_put(&nv_fence->base);
+#else
+            dma_fence_put(&nv_fence->base);
+#endif
 
             nvKms->disableChannelEvent(nv_dev->pDevice,
                                        nv_gem->fenceContext.cb);
@@ -320,7 +376,11 @@
 
         nv_fence = container_of(fence, struct nv_fence, base);
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
         fence_signal(&nv_fence->base);
+#else
+        dma_fence_signal(&nv_fence->base);
+#endif
     }
 }
 
@@ -513,7 +573,11 @@
      * fence_context_alloc() cannot fail, so we do not need to check a return
      * value.
      */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     nv_gem->fenceContext.context = fence_context_alloc(1);
+#else
+    nv_gem->fenceContext.context = dma_fence_context_alloc(1);
+#endif
 
     ret = nvidia_drm_gem_prime_fence_import_semaphore(
               nv_dev, nv_gem, p->index,
@@ -670,7 +734,11 @@
     nv_fence->nv_gem = nv_gem;
 
     spin_lock_init(&nv_fence->lock);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     fence_init(&nv_fence->base, &nvidia_drm_gem_prime_fence_ops,
+#else
+    dma_fence_init(&nv_fence->base, &nvidia_drm_gem_prime_fence_ops,
+#endif
                &nv_fence->lock, nv_gem->fenceContext.context,
                p->sem_thresh);
 
@@ -680,7 +748,11 @@
 
     reservation_object_add_excl_fence(&nv_gem->fenceContext.resv,
                                       &nv_fence->base);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
     fence_put(&nv_fence->base); /* Reservation object has reference */
+#else
+    dma_fence_put(&nv_fence->base); /* Reservation object has reference */
+#endif
 
     ret = 0;
 
--- nvidia-kmod-378.13-x86_64/kernel/nvidia-drm/nvidia-drm-gem.h.orig	2017-02-20 21:52:40.737728898 +0000
+++ nvidia-kmod-378.13-x86_64/kernel/nvidia-drm/nvidia-drm-gem.h	2017-02-20 22:46:40.261335078 +0000
@@ -101,7 +101,11 @@
         /* Software signaling structures */
         struct NvKmsKapiChannelEvent *cb;
         struct nvidia_drm_gem_prime_soft_fence_event_args *cbArgs;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
         struct fence *softFence; /* Fence for software signaling */
+#else
+        struct dma_fence *softFence; /* Fence for software signaling */
+#endif
     } fenceContext;
 #endif
 };
--- nvidia-kmod-378.13-x86_64/kernel/nvidia-drm/nvidia-drm-priv.h.orig	2017-02-20 21:52:56.639326045 +0000
+++ nvidia-kmod-378.13-x86_64/kernel/nvidia-drm/nvidia-drm-priv.h	2017-02-20 22:48:50.308568389 +0000
@@ -34,7 +34,12 @@
 #endif
 
 #if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
+#include <linux/version.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
 #include <linux/fence.h>
+#else
+#include <linux/dma-fence.h>
+#endif
 #include <linux/reservation.h>
 #endif
 
